{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XEjJMnKV_T5N"
   },
   "source": [
    "# An Inference Circuit \n",
    "# (Tror ikke skal være her men i (11))\n",
    "\n",
    "Normally, we would take a look at some learning algorithm, dissect it, and analyze the parts of it to see whether we can find a quantum algorithm that could accelerate that part of the learning protocol. But we can start thinking about learning problems the other way around. We can start from the hardware, looking at the particular quantum computer and what are the kind of things it can naturally do. So we are in this era where quantum computers are imperfect. So we have to factor in these imperfections. And if you look at the actual capabilities, we can develop completely new learning algorithms. So one of the first examples of this kind of thinking was a particular type of kernel for learning that can be executed on a shallow circuit gate model quantum computer. So in this case, we start with a very simple state preparation. And then the only thing we are going to do on this circuit is a Hadamard operation, which will allow us to do interference. So in the first two learning protocols that we looked at, we talked about how you can map a problem to an Ising model. In other words, we used a kind of a Hamiltonian encoding. In this case, we are going to use the amplitude encoding. So if we are given some vector in our data set, which we normalized to one, then we can encode it in the probability amplitudes in a superposition. And so then we have to be careful of how we actually prepare it. But for some data sets, this can be approximated well with shallow circuits. So given this encoding, we can start thinking about new kernels. So the kernel that we are going to calculate is exactly this one. It does not really have a classical analog. It's easy to calculate classically as well, but it's very natural to do on a gate model quantum computer. The shape of the kernel function is going to be something like this. So it's not like the exponential decay of the kernel that you saw in the previous video. It's slightly different, and it might be useful for certain kind of data sets. So the circuit that are going to need, assuming that our data set is only two dimensional, is the following. We have a data qubit. Every single data point is actually going to be included in this single qubit. So this superposition is going to be interesting. Then we have an ancilla qubit, which will be entangled with the test instance that we are trying to calculate the kernel and the data instances that we are given in a training set. Then we have an index qubit, which just keeps track of this index here. And then we have a class qubit, which will contain the label corresponding to a particular data instance. And the protocol is very, very simple. First, you have to prepare a state. The state looks a bit strange. So this is our amplitude encoded test instance, the one for which we're going to calculate the kernel. And here, we have our amplitude encoding data instances. Here's our index register. For bookkeeping, it's also here. Note that these are tensor product states. So these things are not really entangled here. And then we have the ancilla qubit. So the zero state of the ancilla qubit is entangled with the test instance, and the excited state, the one state of the ancilla, is entangled with our data instances. And then to finish it off, we also have the class qubit corresponding to the data instances. So we can think of it as a big black box that does all this preparation. Plus, we have some normalization constant to take care at, you know, this superposition is actually a quantum state. And what we are doing next is nothing but this Hadamard operation here. So since the zero state of the ancilla is entangled with the test state and the excited state is entangled with the data instances, by applying the Hadamard gate again on ancilla, you interfere the data instances with your test instance. So the state that you are going to get will have this form. It will have the test instance plus the data instance. And the test instance minus the data instance is encoded in these vectors. So that's the interference part. And now what we do is we do a measurement on the ancilla if you have a certain probability of success. So by success, I mean that the superposition collapses to this particular part. And based on this, if I forget the output one, then we just discard the result and run the circuit again. And if we get this result so we collapse it to this particular outcome, then we do a measurement in the class qubit as well. And the probability of getting certain results here, we create you exactly this kernel. So the point is that you repeatedly run this algorithm. Sometimes you succeed here. Then you measure here, and based on that, you can calculate this kernel, which could be interesting for a number of applications. \n",
    "\n",
    "• This protocol is attractive because the state preparation step does not include any entangling gates, and therefore it is easy to execute on contemporary quantum computers.\n",
    "\n",
    "– False\n",
    "\n",
    "• We are using amplitude encoding in this protocol. How many qubits would we need to encode four-dimensional vectors?\n",
    "\n",
    "– 2\n",
    "\n",
    "• If we had three data points, would a single-qubit index register still be sufficient?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nVLMGvWHm_5y"
   },
   "source": [
    "# Probalistic Graphical Model\n",
    "\n",
    "Deep learning excels at supervised learning, and it has also been making rapid advances in other programs of machine learning where there are tasks that remain intrinsically difficult to tackle by this paradigm. So there are a couple of problems which are more natural fit to other models-- for instance, probabilistic graphical models. So let's talk about these because as you will see, these can be trained and used efficiently by using quantum resources. So our primary interest is Markov networks. But to get there, let's go through a couple of things. So when we talk about machine learning, we can talk about discriminative problems where the task is this estimation of this conditional probability distribution. So for instance, given an image, tell me what's in the image. So this is where deep learning is excellent. A different task is to learn the actual probability distribution of your data instances, the underlying manifold, or the joint probability distribution with some label. These are hard problems. And there are a couple of instances where you can do this by deep learning, but there are others where you cannot. And probabilistic graphical models are very good at capturing the sparsity structure between random variables. And that way, they model these probability distributions. There are two main types of probabilistic graphical models-- one that the underlying graph is directed. These are called Bayesian networks. So for instance, if you have an observation that your grass is wet, and you have two other random variables which one says that sprinkler was on or off and the other one whether it was raining, then you can make backward inference and ask, hey, given that the grass is wet, what's the probability that it was raining? These are the kind of queries that would be difficult to solve by a neural network. The other large class is called Markov networks, which are undirected. So there's no implication for any kind of causation or direction of causation. So in this case, for instance, we can have three patients who have a certain kind of disease. And we don't know how they infected each other, but we knew that there was some infection pattern going on. So again, we can analyze the network and make statements. So let's take a look how this sparse modeling happens. So what we are after is conditional independence. So we call two random variables conditional independent given a third random variable if you can factorize this joint probability distribution of x and y given z into individual parts, so it could be just px given z and py given z. It doesn't mean that they are independent, but conditioned on this third random variable, they are independent. So for instance, given four random variables, x1 is conditionally independent from x3 and x4 given x2. So this graph, this undirected graph, captures this independent structure or the remaining dependencies. And it turns out that what you can do is define energy functions on the clicks of this graph. So for instance, this is a K1, which means a complete graph on a single node, which is just the node itself, and say that if it takes the value 1, it has some certain energy. And if it takes the value 0, then it has a different energy. Then you can assign some energy value also to configurations over K2's or two nodes that contain an edge or that are connected by an edge. And you see that there's also a K2 here and another one here and another one here. So here, you have a total of four K2's. And finally, you also have this triangle, which is a complete graph on three nodes. It's a click of size 3. And your probability distribution factorizes over these clicks. So as long as you can define an energy function that could model your probability distribution over these clicks, you can describe the full joint probability distribution. And this should look familiar because this has a very similar structure to the thermal state, the state that we get after equilibration in an open quantum system or what we can approximate by the quantum approximate thermalization protocol. In fact, under very mild assumptions, there's a correspondence between Markov networks and the probability distributions they can describe and Boltzmann distributions. And there are a couple of special cases. So when you think about it, the Ising model is a special case. So in this case, our binary-- our random variables are all binary. And we only have K2's. We don't have K3's. And a special case of Ising models and hence Markov networks are Boltzmann machines. Here, you partition your Ising spins into two categories-- one I call the visible ones, and others I called hidden ones. And what you do is you define the same energy function as you would do for an ordinary Markov network. But you're only interested in reproducing some probability distribution here on the visible nodes. So the hidden nodes that only there to help you mitigate correlations between these random variables. So you marginalize out over the hidden nodes to get a probability distribution that you are interested in. These are very powerful methods, and they're expensive to train on classical computers. So this is one area where quantum computers can help. \n",
    "\n",
    "• A discriminative learning model essentially estimates a conditional probability distribution 𝑝(𝑦|𝑥) , whereas a generative model approximates the joint probability distribution 𝑝(𝑥,𝑦) . Why is it harder to learn a generative model?\n",
    "\n",
    "– The output of the learner has the same dimension as the input space, making it harder to create a concise representation.\n",
    "\n",
    "• What's the largest clique size in the following Markov network? \n",
    "# GFX!!!??\n",
    "\n",
    "– 3\n",
    "\n",
    "• What's the maximum clique size in a restricted Boltzmann machine? The restricted Boltzmann machine is the one you see in the video, with no edges within the visible set or the hidden set of nodes.\n",
    "\n",
    "– 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG_XH9Fu9Ma1"
   },
   "source": [
    "The roots of probabilistic graphical models go back to the 1980s, with a strong connection to Bayesian statistics. The story resembles that of neural networks: they have been around for over three decades and they need massive computational power. However, unlike in the case of deep learning, the requirements for computational resources remain out of reach. These models require sampling a distribution, and very often it is the Boltzmann distribution. Since quantum computers can give samples from this distribution, we can hope that quantum hardware can enable these models the same way graphics processing units enabled deep learning.\n",
    "\n",
    "# Probabilistic graphical models\n",
    "\n",
    "Probabilistic graphical models capture a compact representation of a joint probability distribution. For $\\{X_1,\\ldots,X_N\\}$ binary random variables, there are $2^N$ assignments. In a graphical model, complexity is dealt with through graph theory. We get both an efficient treatment of uncertainty (probabilities) and of logical structure (independence constraints). The factorization of the probabilities happens along conditional independences among random variables. The definition is that $X$ is conditionally independent of $Y$ given $Z$ $(X\\perp Y|Z)$, if $P(X=x, Y=y|Z=z) = P(X=x|Z=z)P(Y=y|Z=z)$ for all $x\\in X,y\\in Y,z\\in Z$.\n",
    "\n",
    "The graph can be directed -- these are called Bayesian networks in general -- or undirected, in the case of Markov networks (also known as Markov random fields) [[1](#1)]. Graphical models are quintessentially generative: we explicitly model a probability distribution. Thus generating new samples is trivial and we can always introduce extra random variables to ensure certain properties. These models also take us a step closer to explainability, either by the use of the random variables directly for explanations (if your model is such) or by introducing explanatory random variables that correlate with the others.\n",
    "\n",
    "In a Markov random field, we can allow cycles in the graph and switch from local normalization (conditional probability distribution at each node) to global normalization of probabilities (i.e. a partition function). Examples include countless applications in computer vision, pattern recognition, artificial intelligence, but also Ising models that we have seen before: the factors are defined as degree-1 and degree-2 monomials of the random variables connected in the graph.\n",
    "\n",
    "The factorization is given as a sum $P(X_1, \\ldots, X_N) = \\frac{1}{Z}\\exp(-\\sum_k E[C_k])$, where $C_k$ are are cliques of the graph, and $E[.]$ is an energy defined over the cliques. If $P$ is a Boltzmann distribution over $G$, all local Markov properties will hold. The other way also holds if $P$ is a positive distribution.\n",
    "\n",
    "Let us define a Markov field of binary variables. This will be an Ising model over three nodes. This will contain three cliques of a single node (the on-site fields) and two cliques of two nodes: the edges that connect the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:30.684603Z",
     "start_time": "2018-11-19T20:10:30.190403Z"
    },
    "collapsed": true,
    "id": "fAucCS619MbI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dimod\n",
    "\n",
    "n_spins = 3\n",
    "h = {v: 1 for v in range(n_spins)}\n",
    "J = {(0, 1): 2,\n",
    "     (1, 2): -1}\n",
    "model = dimod.BinaryQuadraticModel(h, J, 0.0, dimod.SPIN)\n",
    "sampler = dimod.SimulatedAnnealingSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGgVJroC9MbK"
   },
   "source": [
    "The probability distribution of a configuration $P(X_1, \\ldots, X_N) = \\frac{1}{Z}\\exp(-\\sum_k E[C_k])$ does not explicitly define the temperature, but it is implicitly there in the constants defining the Hamiltonian. So, for instance, we can scale it by a temperature $T=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvtCO3Yy9MbL"
   },
   "source": [
    "Let's now find out the probability $P(E)$ of each energy level $E$. It can be expressed as a sum over all the states with energy $E$: $P(E)=\\sum_{E(X_1,...,X_n)=E} P(X_1,...,X_N)=\\sum_{E(X_1,...,X_n)=E} \\frac{1}{Z}e^{-E/T}$. The term in the sum is constant (it doesn't depend on $X_1,...,X_N$ anymore). Therefore, we just need to count the number of states such that $E(X_1,...,X_n)=E$. This number is called the *degeneracy* of the energy level $E$, and often noted $g(E)$. Hence, we have $P(E)=\\frac{1}{Z} g(E) e^{-E/T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHvXEn6d9MbM"
   },
   "source": [
    "Let's extract this probability for the particular case of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:32.696067Z",
     "start_time": "2018-11-19T20:10:30.687484Z"
    },
    "id": "fmQS36-29MbM",
    "outputId": "7e377f29-bda4-4313-a932-ae5c4284baf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degeneracy {-4.0: 1, -2.0: 3}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperature = 1\n",
    "response = sampler.sample(model, beta_range=[1/temperature, 1/temperature], num_reads=100)\n",
    "\n",
    "g = {} # dictionary that associate to each energy E the degeneracy g[E]\n",
    "for solution in response.aggregate().data():\n",
    "    if solution.energy in g.keys():\n",
    "        g[solution.energy] += 1\n",
    "    else:\n",
    "        g[solution.energy] = 1\n",
    "print(\"Degeneracy\", g)\n",
    "probabilities = np.array([g[E] * np.exp(-E/temperature) for E in g.keys()])\n",
    "Z = probabilities.sum()\n",
    "probabilities /= Z\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([E for E in g.keys()], probabilities, linewidth=3)\n",
    "ax.set_xlim(min(g.keys()), max(g.keys()))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('Energy')\n",
    "ax.set_ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cW2uawl4n5vR"
   },
   "source": [
    "# Optimization and Sampling PGMs\n",
    "\n",
    "We introduced probabilistic graphical models and Markov networks in particular. And now let's take a look at what other kind of queries that you can ask from these networks. The easiest query that you can run is called the most probable explanation. In this case, you have some evidence you observe some of your random variables. For instance, these two random variables would consist the set which belong to the evidence. For instance, this random variable takes the value one, and this takes the value zero. And then the question you are asking is that given this evidence, what's the most likely configuration for the rest of the network? So you are looking for the arg max of the remaining variables in the graph. Then the second one, and somewhat more complicated one, is when you are, again, given the evidence-- so you clamp some of these random variables. But you're not interested in the configuration of all of the remaining random variables. So for instance, you are not interested in what this one is doing. So you're are only interested in just some subset. So you marginalize over those variables that you're not interested in, and you're only looking for an optimal configuration over a subset that you are interested in. So think about things like having a probability distribution describing different symptoms and their correlations. And you have a patient, then you observe a couple of those symptoms but not others. Then you can run a query to find out all the other symptoms that you shouldn't be looking for assuming that the person has a certain kind of disease. And the main thing here is that even if you train the network and it reproduces the probability distribution that you're interested in, running these queries is still computationally very difficult. In fact, most of these problems are at least np hard. When you think about deep learning, once you train the network, running an inference step is relatively inexpensive. You can run it on a cell phone in many cases. Whereas here, even inference, using the model, remains hard. So if you can give some advantage by using quantum resources, that's extremely valuable. So given that the problem is hard, there are a couple of ways of dealing with it. One of the ways of dealing with it is using approximate inference. So instead of solving it accurately, you run some sampling over possible outcomes. And then you have a digital computer, that's deterministic. You actually fake randomness. And you fake sampling from a probability distribution by methods which are typically Markov chain Monte Carlo methods which have their own problems. They take time to burn in, and samples can be correlated. And first and foremost, they take a lot of computational time. But then remember that the probability distribution is just this. It factorizes a Boltzmann distribution. So if you can set an Ising model, then we can just use a quantum computer to do approximate inference for us. And you have options. So if you have a quantum annealer and you have a fair number of qubits and your task is to estimate the effective temperature of the system, so you can rescale these energy values according to what the hardware is actually executing. So there is this extra step that you have to do. Or you can run the gate model quantum approximate thermalization protocol, which needs an ancilla system, which means that you have to use a lot more qubits than what you are just actually interested in. And then on a smaller scale, well, that's an option too. Whichever method you choose, you can accelerate some of these Markov networks at training and also during inference. \n",
    "\n",
    "• Markov chain Monte Carlo methods give you a way to sample a probability distribution. What are some of their common drawbacks?\n",
    "\n",
    "– (Answer Incorrect: True randomness requires purpose-built hardware in classical computers. There is vast literature on how to sample arbitrary distributions with these methods. If you pull out samples from the same chain one after the othr, there will be correlations between them. Submit)\n",
    "\n",
    "– (Answer Incorrect:True randomness requires purpose-built hardware in classical computers. Markov chains have a burn-in time before you can sample them.\n",
    "\n",
    "– They use pseudo random number generators on classical computers.\n",
    "\n",
    "– They can be computationally demanding.\n",
    "\n",
    "– Samples can be correlated.\n",
    "\n",
    "• If you have a problem that have two solutions with equal energy (implying equal probability), are they in general equally likely to reach with Monte Carlo methods? Remember that the configuration of random variables for the same energy level can be very different.\n",
    "\n",
    "– False\n",
    "\n",
    "• Are the samples going to be correlated if they come from a quantum device?\n",
    "\n",
    "– No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41oVcL6A9MbP"
   },
   "source": [
    "In this case, the conditional independences are already encapsulated by the model: for instances, spins 0 and 2 do not interact. In general, it is hard to learn the structure of a probabilistic graphical given a set of observed correlations in the sample $S$. We can only rely on heuristics. The typical way of doing it is to define a scoring function and do some heuristic global optimization. \n",
    "\n",
    "Once we identified or defined the graph structure $G$, we have to learn the probabilities in the graph. We again rely on our sample and its correlations, and use a maximum likelihood or a maximum a posteriori estimate of the corresponding parameters $\\theta_G$ with the likelihood $P(S|\\theta_G)$. This is again a hard problem.\n",
    "\n",
    "Applying the learned model means probabilistic inference to answer queries of the following types:\n",
    "\n",
    "-   Conditional probability: $P(Y|E=e)=\\frac{P(Y,e)}{P(e)}$.\n",
    "\n",
    "-   Maximum a posteriori:\n",
    "    $\\mathrm{argmax}_y P(y|e)=\\mathrm{argmax}_y \\sum_Z P(y, Z|e)$.\n",
    "\n",
    "This problem is in \\#P. Contrast this to deep learning: once the neural network is trained, running a prediction on it is relatively cheap. In the case of probabilistic graphical models, inference remains computationally demanding even after training the model. Instead of solving the inference problem directly, we use approximate inference with sampling, which is primarily done with Monte Carlo methods on a classical computer. These have their own problems of slow burn-in time and correlated samples, and this is exactly the step we can replace by sampling on a quantum computer.\n",
    "\n",
    "For instance, let us do a maximum a posteriori inference on our Ising model. We clamp the first spin to -1 and run simulated annealing for the rest of them to find the optimal configuration. We modify the simulated annealing routine in `dimod` to account for the clamping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:32.992517Z",
     "start_time": "2018-11-19T20:10:32.705659Z"
    },
    "collapsed": true,
    "id": "WceaKF3E9MbS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dimod.reference.samplers.simulated_annealing import greedy_coloring\n",
    "\n",
    "clamped_spins = {0: -1}\n",
    "num_sweeps = 1000\n",
    "βs = [1.0 - 1.0*i / (num_sweeps - 1.) for i in range(num_sweeps)]\n",
    "\n",
    "# Set up the adjacency matrix.\n",
    "adj = {n: set() for n in h}\n",
    "for n0, n1 in J:\n",
    "    adj[n0].add(n1)\n",
    "    adj[n1].add(n0)\n",
    "# Use a vertex coloring for the graph and update the nodes by color\n",
    "__, colors = greedy_coloring(adj)\n",
    "\n",
    "spins = {v: np.random.choice((-1, 1)) if v not in clamped_spins else clamped_spins[v]\n",
    "         for v in h}\n",
    "for β in βs:\n",
    "    energy_diff_h = {v: -2 * spins[v] * h[v] for v in h}\n",
    "\n",
    "    # for each color, do updates\n",
    "    for color in colors:\n",
    "        nodes = colors[color]\n",
    "        energy_diff_J = {}\n",
    "        for v0 in nodes:\n",
    "            ediff = 0\n",
    "            for v1 in adj[v0]:\n",
    "                if (v0, v1) in J:\n",
    "                    ediff += spins[v0] * spins[v1] * J[(v0, v1)]\n",
    "                if (v1, v0) in J:\n",
    "                    ediff += spins[v0] * spins[v1] * J[(v1, v0)]\n",
    "\n",
    "            energy_diff_J[v0] = -2. * ediff\n",
    "        for v in filter(lambda x: x not in clamped_spins, nodes):\n",
    "            logp = np.log(np.random.uniform(0, 1))\n",
    "            if logp < -1. * β * (energy_diff_h[v] + energy_diff_J[v]):\n",
    "                spins[v] *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_PdUTMt9MbT"
   },
   "source": [
    "Running this algorithm, we can obtain the most likely configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:33.018780Z",
     "start_time": "2018-11-19T20:10:32.997312Z"
    },
    "id": "7oPGA3Pe9MbX",
    "outputId": "b42a3ab1-80c1-4eaa-e84b-f5c17f303e1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -1, 1: -1, 2: -1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeYtzCvz_p_m"
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "y8vTiuM7_r4w"
   },
   "source": [
    "# Se igjen i lyx (Husk implementering og plots)\n",
    "\n",
    "Deep learning excels at supervised learning, and it has also been making rapid advances in other programs of machine learning where there are tasks that remain intrinsically difficult to tackle by this paradigm. So there are a couple of problems which are more natural fit to other models-- for instance, probabilistic graphical models. So let's talk about these because as you will see, these can be trained and used efficiently by using quantum resources. So our primary interest is Markov networks. But to get there, let's go through a couple of things. So when we talk about machine learning, we can talk about discriminative problems where the task is this estimation of this conditional probability distribution. So for instance, given an image, tell me what's in the image. So this is where deep learning is excellent. A different task is to learn the actual probability distribution of your data instances, the underlying manifold, or the joint probability distribution with some label. These are hard problems. And there are a couple of instances where you can do this by deep learning, but there are others where you cannot. And probabilistic graphical models are very good at capturing the sparsity structure between random variables. And that way, they model these probability distributions. There are two main types of probabilistic graphical models-- one that the underlying graph is directed. These are called Bayesian networks. So for instance, if you have an observation that your grass is wet, and you have two other random variables which one says that sprinkler was on or off and the other one whether it was raining, then you can make backward inference and ask, hey, given that the grass is wet, what's the probability that it was raining? These are the kind of queries that would be difficult to solve by a neural network. The other large class is called Markov networks, which are undirected. So there's no implication for any kind of causation or direction of causation. So in this case, for instance, we can have three patients who have a certain kind of disease. And we don't know how they infected each other, but we knew that there was some infection pattern going on. So again, we can analyze the network and make statements. So let's take a look how this sparse modeling happens. So what we are after is conditional independence. So we call two random variables conditional independent given a third random variable if you can factorize this joint probability distribution of x and y given z into individual parts, so it could be just px given z and py given z. It doesn't mean that they are independent, but conditioned on this third random variable, they are independent. So for instance, given four random variables, x1 is conditionally independent from x3 and x4 given x2. So this graph, this undirected graph, captures this independent structure or the remaining dependencies. And it turns out that what you can do is define energy functions on the clicks of this graph. So for instance, this is a K1, which means a complete graph on a single node, which is just the node itself, and say that if it takes the value 1, it has some certain energy. And if it takes the value 0, then it has a different energy. Then you can assign some energy value also to configurations over K2's or two nodes that contain an edge or that are connected by an edge. And you see that there's also a K2 here and another one here and another one here. So here, you have a total of four K2's. And finally, you also have this triangle, which is a complete graph on three nodes. It's a click of size 3. And your probability distribution factorizes over these clicks. So as long as you can define an energy function that could model your probability distribution over these clicks, you can describe the full joint probability distribution. And this should look familiar because this has a very similar structure to the thermal state, the state that we get after equilibration in an open quantum system or what we can approximate by the quantum approximate thermalization protocol. In fact, under very mild assumptions, there's a correspondence between Markov networks and the probability distributions they can describe and Boltzmann distributions. And there are a couple of special cases. So when you think about it, the Ising model is a special case. So in this case, our binary-- our random variables are all binary. And we only have K2's. We don't have K3's. And a special case of Ising models and hence Markov networks are Boltzmann machines. Here, you partition your Ising spins into two categories-- one I call the visible ones, and others I called hidden ones. And what you do is you define the same energy function as you would do for an ordinary Markov network. But you're only interested in reproducing some probability distribution here on the visible nodes. So the hidden nodes that only there to help you mitigate correlations between these random variables. So you marginalize out over the hidden nodes to get a probability distribution that you are interested in. These are very powerful methods, and they're expensive to train on classical computers. So this is one area where quantum computers can help. \n",
    "\n",
    "• A discriminative learning model essentially estimates a conditional probability distribution 𝑝(𝑦|𝑥) , whereas a generative model approximates the joint probability distribution 𝑝(𝑥,𝑦) . Why is it harder to learn a generative model?\n",
    "\n",
    "– The output of the learner has the same dimension as the input space, making it harder to create a concise representation.\n",
    "\n",
    "• What's the largest clique size in the following Markov network? \n",
    "# Se bildet grafen på lyx\n",
    "\n",
    "– 3\n",
    "\n",
    "• What's the maximum clique size in a restricted Boltzmann machine? The restricted Boltzmann machine is the one you see in the video, with no edges within the visible set or the hidden set of nodes.\n",
    "\n",
    "– 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZsTrDTU9MbY"
   },
   "source": [
    "# Boltzmann machines\n",
    "\n",
    "A Boltzmann machine generates samples from a probability distributition $P(\\textbf{v})$ inferred from the data, where $\\textbf{v} \\in \\{0,1\\}^n$. The assumption is that this distribution lies on a latent space that can be paramerized by a set of hidden variables $\\textbf{h} \\in \\{0,1\\}^n$, such that $P(\\textbf{v})=\\sum_h P(\\textbf{v}|\\textbf{h})P(\\textbf{h})$. The joint probability distribution is modeled as a Gibbs distribution with the energy defined by an Ising Model: $P(\\textbf{v}, \\textbf{h})=\\frac{1}{Z} e^{-\\beta E(\\textbf{h},\\textbf{v})}$ and $E(\\textbf{h},\\textbf{v})=-\\sum_{i,j} W_{ij} h_i v_j$. It can then be shown that $p(\\textbf{h}|\\textbf{v})=\\sigma(W \\cdot \\textbf{v})$ and $p(\\textbf{v}|\\textbf{h})=\\sigma(W \\cdot \\textbf{h})$, where $\\sigma$ is the sigmoid function defined by $\\sigma(x)=\\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "To train a Boltzmann machine, we look for the weights $W$ that maximizes the log-likelihood $L=\\sum_{\\textbf{v} \\in S} \\log(p(\\textbf{v}|W))$, where $S$ is the training set. This function can be optimized using regular gradient ascent: $W_{ij}^{(t+1)}=W_{ij}^{(t)} + \\eta \\frac{\\partial L}{\\partial W_{ij}}$. Computing the gradient $\\frac{\\partial L}{\\partial W_{ij}}$ is the hard part. Indeed, we can show that \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}}=\\frac{1}{|S|} \\sum_{\\textbf{v} \\in S} \\mathbb{E}_{\\textbf{h} \\sim P(\\textbf{h}|\\textbf{v})}[h_i v_j] - \\mathbb{E}_{(\\textbf{h},\\textbf{v}) \\sim P(\\textbf{h},\\textbf{v})}[h_i v_j]$$.\n",
    "\n",
    "The first expectation value is easy to compute: it is equal to $\\sigma \\left( \\sum_j W_{ij} v_j \\right) v_j$. We only need to sum those expectation values over the dataset. This is called the positive phase, after its positive sign in the gradient.\n",
    "\n",
    "The second expectation value cannot be simplified as easily, since it is taken over all possible configuration $\\textbf{v}$ and $\\textbf{h}$. It would take an exponential amount of time to compute it exactly. We can use the exact same quantum sampling method as above to outsource this part of the calculation to a quantum processing unit and train Boltzmann machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zF9KGOh99Mba"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] Koller, D., Friedman, N., Getoor, L., Taskar, B. (2007). [Graphical Models in a Nutshell](https://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf). In *Introduction to Statistical Relational Learning*, MIT Press. <a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0QUhuO7AekI"
   },
   "source": [
    "# SE EKSAMEN!!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_4_QML_Classical-Quantum Hybrid Learning Algorithms_(12):_Training_Probabilistic_Graphical_Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}